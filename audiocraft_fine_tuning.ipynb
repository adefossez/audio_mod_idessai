{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwkqG4vQ4roA"
   },
   "source": [
    "# Modelisation of audio with descrite tokens.\n",
    "\n",
    "In this notebook, we will use [AudioCraft](https://github.com/facebookresearch/audiocraft)\n",
    "in order fine tune a [MusicGen](https://arxiv.org/abs/2306.05284) model on a custom dataset of audio.\n",
    "\n",
    "We have limited resources, so we will stick to a 300M parameter model. We will also use float32 precision, meaning our base memory usage is 300M * 4 (float size) * 4 (model + grad + momemtum + adam denominator) = 4.8 GB.\n",
    "If we would want to fine tune the 1.5B model, that would be 24GB base memory requirements.\n",
    "\n",
    "Note this is only the \"base\" memory requirements, without accounting for activations. That's where activation checkpointing comes in handy as we will see.\n",
    "\n",
    "We could lower the this usage by using [LoRA](https://arxiv.org/abs/2106.09685) but we will start simple and stick to the 300M.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m audio_mod_idessai.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKGez3lRYts1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from audiocraft.models import loaders\n",
    "\n",
    "# Let's load a pretrained Encodec model.\n",
    "compression_model = loaders.load_compression_model('facebook/musicgen-small', device='cuda')\n",
    "compression_model.eval();\n",
    "# This model operates at 50 Hz, with a bandwidth of 2kbps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "id": "40kxRsv3ZA6y",
    "outputId": "57bab560-0194-4013-c57c-ab5be8eccdf8"
   },
   "outputs": [],
   "source": [
    "from audiocraft.data.audio_dataset import load_audio_meta\n",
    "from audiocraft.data.music_dataset import MusicDataset\n",
    "from audiocraft.utils.utils import get_loader\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "from audio_mod_idessai import config\n",
    "\n",
    "meta = load_audio_meta(config.EGS_FILE)\n",
    "dset = MusicDataset(meta, segment_duration=15., shuffle=True, sample_rate=32000, channels=1, min_segment_ratio=0.8, return_info=True,\n",
    "                    num_samples=1_000_000)\n",
    "\n",
    "wav, info = dset[0]\n",
    "display_audio(wav, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "piia1VtJbovg",
    "outputId": "9d064554-2693-413c-dadf-b914b51141d8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    codes, _ = compression_model.encode(wav.cuda()[None])\n",
    "    decoded = compression_model.decode(codes)\n",
    "    print(codes.shape)\n",
    "    display_audio(wav, 32000)\n",
    "    display_audio(decoded, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mx7UPBGZgnUm",
    "outputId": "873af7ff-983a-455c-af13-c61d3db708d0"
   },
   "outputs": [],
   "source": [
    "from flashy.utils import averager\n",
    "from audio_mod_idessai import utils\n",
    "import time\n",
    "\n",
    "def _apply_layer(layer, *args, **kwargs):\n",
    "    return utils.simple_checkpoint(layer, *args, **kwargs)\n",
    "\n",
    "init = True\n",
    "lora = False\n",
    "if init:\n",
    "    if 'lm' in globals():\n",
    "        del lm\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "if init:\n",
    "    loader = get_loader(dset, 1000000, batch_size=12, num_workers=4, seed=0, collate_fn=dset.collater, shuffle=True)\n",
    "    lm = loaders.load_lm_model('facebook/musicgen-small', device='cuda')\n",
    "    lm.to(dtype=torch.float)\n",
    "    # Activation checkpointing is required to allow blowing up memory, at the cost of making\n",
    "    # two forwards: one extra during the backward. Divides memory requirements by\n",
    "    # the number of layers...\n",
    "    lm.transformer.checkpointing = 'torch'\n",
    "    # lm.transformer._apply_layer = _apply_layer\n",
    "    if lora:\n",
    "        for layer in lm.transformer.layers:\n",
    "            utils.add_lora_(layer)\n",
    "    # Always use AdamW with weight_decay, and weight decay is pretty much mandatory for Transformers.\n",
    "    opt = torch.optim.AdamW(lm.parameters(), lr=1e-5, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "niters = 100_000\n",
    "avg = averager(0.99)\n",
    "\n",
    "def do_one(wav, infos):\n",
    "    wav = wav.cuda()\n",
    "    with torch.no_grad():\n",
    "        codes, _ = compression_model.encode(wav)\n",
    "    for info in infos:\n",
    "        info.description = 'A techno track.'\n",
    "    cas = [info.to_condition_attributes() for info in infos]\n",
    "    res = lm.compute_predictions(codes, cas)\n",
    "    ce = utils.compute_cross_entropy(res.logits, codes, res.mask)\n",
    "    ce_tot = ce.sum() / res.mask.sum()\n",
    "    ce_tot.backward()\n",
    "    grad_norm = sum(p.grad.data.norm(p=2).pow(2) for p in lm.parameters() if p.grad is not None).sqrt()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    return {\n",
    "        'ce': ce_tot.detach(),\n",
    "        'gn': grad_norm\n",
    "    }\n",
    "\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# loader = [batch] * niters\n",
    "\n",
    "last_step = 0\n",
    "last_time = time.time()\n",
    "for idx, batch in enumerate(loader):\n",
    "    if idx == niters:\n",
    "        break\n",
    "    wav, infos = batch\n",
    "    metrics = do_one(wav, infos)\n",
    "    ametrics = avg(metrics)\n",
    "\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        new_time = time.time()\n",
    "        speed = (new_time - last_time) / (idx + 1 - last_step)\n",
    "        last_time = new_time\n",
    "        last_step = idx + 1\n",
    "        mx_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "        print(f\"{idx + 1: 6d}: ce={ametrics['ce']:.5f} gn={ametrics['gn']:.3f}, mx_mem={mx_mem:.1f}GB, spd={speed:.1f} btch/s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "m10Sm2-gvaq4",
    "outputId": "5c1e385f-b2c4-4ba8-f424-b6713275b936"
   },
   "outputs": [],
   "source": [
    "# MusicGen is just a wrapper to make generation easier!\n",
    "from audiocraft.models.musicgen import MusicGen\n",
    "\n",
    "def test_gen():\n",
    "    mg = MusicGen('test', compression_model, lm, max_duration=30)\n",
    "    gen = mg.generate_unconditional(4, progress=True)\n",
    "    return gen\n",
    "\n",
    "out = test_gen()\n",
    "display_audio(out, 32000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
